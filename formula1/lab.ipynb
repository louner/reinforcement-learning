{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sample_bot import *\n",
    "import shutil\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import socketio\n",
    "import eventlet\n",
    "import eventlet.wsgi\n",
    "from flask import Flask\n",
    "\n",
    "import math\n",
    "from queue import Queue\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from random import random, choice\n",
    "\n",
    "from time import sleep\n",
    "#import pyautogui\n",
    "\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_size = batch_size*10\n",
    "t_max = 5\n",
    "reload_window = 100\n",
    "reward_decay = 0.99\n",
    "last_state = None\n",
    "last_action = None\n",
    "last_reward = None\n",
    "learning_rate = 0.001\n",
    "mu = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 possible steering angles, 0 and 5 uniformly selected angle from 0~pi and 0~-1*pi\n",
    "#ANGLES = [-3.141592653589793, -2.5132741228718345, -1.8849555921538759, -1.2566370614359172, -0.6283185307179586, 0, 0.6283185307179586, 1.2566370614359172, 1.8849555921538759, 2.5132741228718345, 3.141592653589793]\n",
    "ANGLES = [-0.349065850398866, -0.3316125578789227, -0.31415926535897937, -0.29670597283903605, -0.2792526803190927, -0.2617993877991494, -0.24434609527920612, -0.22689280275926282, -0.20943951023931953, -0.19198621771937624, -0.17453292519943295, -0.15707963267948966, -0.13962634015954636, -0.12217304763960306, -0.10471975511965977, -0.08726646259971647, -0.06981317007977318, -0.05235987755982989, -0.03490658503988659, -0.017453292519943295, 0, 0.017453292519943295, 0.03490658503988659, 0.05235987755982989, 0.06981317007977318, 0.08726646259971647, 0.10471975511965977, 0.12217304763960306, 0.13962634015954636, 0.15707963267948966, 0.17453292519943295, 0.19198621771937624, 0.20943951023931953, 0.22689280275926282, 0.24434609527920612, 0.2617993877991494, 0.2792526803190927, 0.29670597283903605, 0.31415926535897937, 0.3316125578789227, 0.349065850398866]\n",
    "#ANGLES = [0.0]\n",
    "# 3 possible throttle options\n",
    "THROTTLES = [0.0, 0.5, 1.0]\n",
    "#THROTTLES = [0.0, 0.1]\n",
    "ACTIONS = [(angle, throttle) for angle in ANGLES for throttle in THROTTLES] #33 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_control(steering_angle, throttle):\n",
    "    sio.emit(\n",
    "        \"steer\",\n",
    "        data={\n",
    "            'steering_angle': str(steering_angle),\n",
    "            'throttle': str(throttle)\n",
    "        },  \n",
    "        skip_sid=True)\n",
    "    \n",
    "def calculate_avg(series):\n",
    "    return [np.mean(serie) for serie in series]\n",
    "\n",
    "def plot(series):\n",
    "    colors = ['r', 'y', 'g', 'b', 'p']\n",
    "    to_plot = []\n",
    "    for i in range(len(series)):\n",
    "        to_plot += [series[i], colors[i]]\n",
    "        \n",
    "    plt.plot(*to_plot)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "#plot([[x for x in range(10)], [x+1 for x in range(10)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, action_idx):\n",
    "        self.action_idx = action_idx\n",
    "        self.steering_angle = ACTIONS[action_idx][0]\n",
    "        self.throttle = ACTIONS[action_idx][1]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str({'throttle': self.throttle, 'angle': self.steering_angle})\n",
    "\n",
    "class Transaction:\n",
    "    columns = ['state', 'action', 'reward', 'next_state', 'advantage']\n",
    "    \n",
    "    def __init__(self, item):\n",
    "        self.item = dict(zip(Transaction.columns, item))\n",
    "    \n",
    "    def  __lt__(self, other):\n",
    "        #sort by time\n",
    "        return self.item['id'] < other.item['id']\n",
    "        #sort by gained label\n",
    "        #return self.item['label'] < other.item['label']\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.item)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self.item[key]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.item[key] = value\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        MAX_TRANSACTION = Transaction(Transaction.columns)\n",
    "        MAX_TRANSACTION.item['id'] = -1\n",
    "        self.d = []\n",
    "        self.id = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def insert(self, item):\n",
    "        self.id += 1\n",
    "        item['id'] = self.id\n",
    "        #print(self.d)\n",
    "\n",
    "        if len(self.d) == self.max_size:\n",
    "            heapq.heapreplace(self.d, item)\n",
    "        else:\n",
    "            heapq.heappush(self.d, item)\n",
    "\n",
    "    def batch(self, n):\n",
    "        if len(self.d) > n:\n",
    "            batch = np.array(sample(self.d, n))\n",
    "            return batch\n",
    "        else:\n",
    "            return np.array(self.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy & value network only different at last layer\n",
    "# common part is cnn1-cnn1-fc\n",
    "def build_network():\n",
    "    with tf.variable_scope('Action-Critic-Network-Common', reuse=tf.AUTO_REUSE):\n",
    "        #inputs = tf.placeholder(shape=(None, 108, 320, 3), dtype=tf.float32) # track image\n",
    "        inputs = tf.placeholder(shape=(None, 240, 320, 3), dtype=tf.float32) # track image\n",
    "        layer_1 = slim.conv2d(inputs=inputs, num_outputs=16, kernel_size=(8, 8), stride=(8, 8), padding='VALID', activation_fn=tf.nn.elu)\n",
    "        layer_2 = slim.conv2d(inputs=layer_1, num_outputs=32, kernel_size=(4, 4), stride=(4, 4), padding='VALID', activation_fn=tf.nn.elu)\n",
    "        fc_layer = tf.layers.dense(inputs=slim.flatten(layer_2), units=256, activation=tf.nn.relu)\n",
    "    return inputs, fc_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(state) = action probability\n",
    "class PolicyNetwork:\n",
    "    def __init__(self):\n",
    "        self.action_size = len(ACTIONS)\n",
    "        \n",
    "        self.advantage = tf.placeholder(shape=(None, self.action_size), dtype=tf.float32)\n",
    "        \n",
    "        inputs, output = build_network()\n",
    "        self.inputs = inputs\n",
    "    \n",
    "        self.metrics = {}\n",
    "        self.metrics['loss'] = []\n",
    "        \n",
    "        with tf.variable_scope('policy-network'):\n",
    "            self.actions_values = tf.layers.dense(output, units=self.action_size, activation=tf.nn.softmax) # batch_size, action_size\n",
    "            self.best_action = tf.argmax(self.actions_values, axis=1)\n",
    "\n",
    "            self.loss = tf.reduce_sum(tf.log(self.actions_values) * self.advantage)*-1\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        best_action = self.sess.run(self.best_action, feed_dict={self.inputs: [state]})[0]\n",
    "        return best_action\n",
    "    \n",
    "    def update(self, batch):\n",
    "        states = [transaction['state'] for transaction in batch]\n",
    "        advantages = []\n",
    "        for transaction in batch:\n",
    "            # unselected actions get 0 advantage, selected action get reward - current_state_value advantage\n",
    "            advantage = [0]*self.action_size\n",
    "            advantage[transaction['action']] = transaction['advantage']\n",
    "\n",
    "            advantages.append(advantage)\n",
    "            \n",
    "        advantages = np.array(advantages)\n",
    "        \n",
    "        _, loss = self.sess.run([self.train_op, self.loss], feed_dict={self.inputs: states, self.advantage: advantages})\n",
    "        self.metrics['loss'].append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V(state) = state_value\n",
    "class ValueNetwork:\n",
    "    def __init__(self):\n",
    "        self.output_size = 1\n",
    "        self.model_folder = './model/value_network'\n",
    "        \n",
    "        self.rewards = tf.placeholder(shape=(None, 1), dtype=tf.float32)\n",
    "        inputs, output = build_network()\n",
    "        \n",
    "        with tf.variable_scope('value-network'):\n",
    "            self.inputs = inputs\n",
    "            self.state_value = tf.layers.dense(output, units=self.output_size)\n",
    "\n",
    "            self.loss = tf.reduce_sum(tf.square(self.state_value - self.rewards))\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.target_sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.target_sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        self.metrics = {}\n",
    "        self.metrics['loss'] = []\n",
    "    \n",
    "    def get_state_values(self, states):\n",
    "        state_values = self.target_sess.run(self.state_value, feed_dict={self.inputs: states})\n",
    "        return state_values\n",
    "    \n",
    "    def get_state_value(self, state):\n",
    "        return self.get_state_values([state])[0]\n",
    "    \n",
    "    def update(self, batch):\n",
    "        inputs = [transaction['state'] for transaction in batch]\n",
    "        rewards = [transaction['reward'] for transaction in batch]\n",
    "        \n",
    "        _, loss = self.sess.run([self.train_op, self.loss], feed_dict={self.inputs: inputs, self.rewards: rewards})\n",
    "        self.metrics['loss'].append(loss)\n",
    "    \n",
    "    def reload(self, model_name):\n",
    "        self.saver.save(self.sess, '%s/%s'%(self.model_folder, model_name))\n",
    "        self.saver.restore(self.target_sess, tf.train.latest_checkpoint(self.model_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when t_max transactions are collected, calculate first transaction's reward/advantage\n",
    "def add_advantage(transaction, transactions, value_network):\n",
    "    final_state = transactions[-1]['state']\n",
    "    final_state_value = value_network.get_state_value(final_state)\n",
    "    \n",
    "    reward = final_state_value\n",
    "    for t in reversed(transactions):\n",
    "        reward += t['reward']\n",
    "        reward *= reward_decay\n",
    "        \n",
    "    transaction['reward'] = reward\n",
    "    \n",
    "    current_state = transaction['state']\n",
    "    current_state_value = value_network.get_state_value(current_state)\n",
    "    \n",
    "    advantage = reward - current_state_value\n",
    "    transaction['advantage'] = advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACNDrive(AutoDrive):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        AutoDrive.__init__(self, *args, **kwargs)\n",
    "        \n",
    "        self.policy_network = PolicyNetwork()\n",
    "        self.value_network = ValueNetwork()\n",
    "        \n",
    "        self.init_game()\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.metrics = {}\n",
    "        self.metrics['round_length'] = [] # number of games, 1\n",
    "        self.metrics['rewards'] = [] # number of games, game round length\n",
    "        self.metrics['speeds'] = [] # number of games, game round length\n",
    "\n",
    "    def init_game(self):\n",
    "        self.memory = Memory(max_size=max_size)\n",
    "        self.transactions = []\n",
    "        self.round_id = 1\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "        \n",
    "        self.mu = 0.999\n",
    "        self.speeds_batch = [1]*10\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.speeds = []\n",
    "        \n",
    "    def collect_game_metrics(self):\n",
    "        self.metrics['round_length'].append(self.round_id)\n",
    "        self.metrics['rewards'].append(self.rewards)\n",
    "        self.metrics['speeds'].append(self.speeds)\n",
    "        \n",
    "        self.reward_history += self.rewards\n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        avg_reward = calculate_avg(self.metrics['rewards'])\n",
    "        avg_speed = calculate_avg(self.metrics['speeds'])\n",
    "        avg_round_length = calculate_avg(self.metrics['round_length'])\n",
    "        \n",
    "        plot([avg_reward, avg_speed, avg_round_length])\n",
    "        \n",
    "    def update_speeds(self, speed):\n",
    "        self.speeds_batch = self.speeds_batch[1:]\n",
    "        self.speeds_batch.append(speed)\n",
    "\n",
    "    def is_crash(self):\n",
    "        return np.mean(self.speeds_batch) <= 0.01\n",
    "    \n",
    "    def calculate_reward(self, steering_angle, speed):\n",
    "        return speed * math.cos(steering_angle)\n",
    "\n",
    "    def end_and_restart_new_game(self):\n",
    "        print('crashed, restarting...')\n",
    "        pyautogui.press('esc')\n",
    "        # my mouse ...\n",
    "        for _ in range(10):\n",
    "            pyautogui.click()\n",
    "        \n",
    "        self.collect_game_metrics()\n",
    "        self.plot_metrics()\n",
    "        self.init_game()\n",
    "\n",
    "    def on_dashboard(self, src_img, last_steering_angle, speed, throttle, info):        \n",
    "        # get angle to calculate reward\n",
    "        self.update_speeds(speed)\n",
    "        if self.is_crash():\n",
    "            self.end_and_restart_new_game()\n",
    "\n",
    "        try:\n",
    "            track_img     = ImageProcessor.preprocess(src_img)\n",
    "            current_angle = ImageProcessor.find_steering_angle_by_color(track_img, last_steering_angle, debug = self.debug)\n",
    "            steering_angle = self._steering_pid.update(-current_angle)\n",
    "            throttle       = self._throttle_pid.update(speed)\n",
    "            \n",
    "        except:\n",
    "            print(traceback.format_exc())\n",
    "            print(self.speeds_batch)\n",
    "            self._car.control(0, 0)\n",
    "            return\n",
    "\n",
    "        if True:\n",
    "            ImageProcessor.show_image(src_img, \"source\")\n",
    "            ImageProcessor.show_image(track_img, \"track\")\n",
    "            logit(\"steering PID: %0.2f (%0.2f) => %0.2f (%0.2f)\" % (current_angle, ImageProcessor.rad2deg(current_angle), steering_angle, ImageProcessor.rad2deg(steering_angle)))\n",
    "            logit(\"throttle PID: %0.4f => %0.4f\" % (speed, throttle))\n",
    "            logit(\"info: %s\" % repr(info))\n",
    "        \n",
    "        # update last transaction\n",
    "        # because can't get next state right after executing action QQ\n",
    "        state = src_img\n",
    "        if self.last_state is not None:\n",
    "            transaction = Transaction([self.last_state, self.last_action, self.last_reward, state, None])\n",
    "            self.transactions.append(transaction)\n",
    "        \n",
    "        # select action according to current state\n",
    "        action = self.policy_network.get_action(state)\n",
    "        \n",
    "        if random() < self.mu:\n",
    "            action = randint(a=0, b=len(ACTIONS)-1)\n",
    "        action = Action(action)\n",
    "        steering_angle, throttle = action.steering_angle, action.throttle\n",
    "        \n",
    "        self.last_action = action.action_idx\n",
    "        self.last_reward = self.calculate_reward(steering_angle, speed)\n",
    "        self.last_state = state\n",
    "        \n",
    "        self.rewards.append(self.last_reward)\n",
    "        self.speeds.append(speed)\n",
    "        \n",
    "        plot([self.speeds, self.rewards])\n",
    "        \n",
    "        # execute action\n",
    "        self._car.control(steering_angle, throttle)\n",
    "        \n",
    "        print(len(drive.memory.d), len(self.transactions), self.last_reward, steering_angle, self.speeds_batch)\n",
    "        \n",
    "        # update policy & value network\n",
    "        batch = self.memory.batch(batch_size)\n",
    "        if batch.any():\n",
    "            self.policy_network.update(batch)\n",
    "            self.value_network.update(batch)\n",
    "        \n",
    "        # when storing t_max transactions, update first transaction's label, pop & push it into memory\n",
    "        if len(self.transactions) == t_max:\n",
    "            transaction = self.transactions[0]\n",
    "            add_advantage(transaction, self.transactions, self.value_network)\n",
    "            self.memory.insert(transaction)\n",
    "            self.transactions = self.transactions[1:]\n",
    "        \n",
    "        # reload policy & value network periodically\n",
    "        if self.round_id % reload_window == 0:\n",
    "            #policy_network.reload() #policy network need reload ?\n",
    "            self.value_network.reload(str(info))\n",
    "            \n",
    "        self.round_id += 1\n",
    "        #self.mu *= self.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sio = socketio.Server()\n",
    "record_folder = './records/'\n",
    "car = Car(control_function = send_control)\n",
    "#drive = AutoDrive(car, record_folder=record_folder)\n",
    "drive = ACNDrive(car, record_folder=record_folder)\n",
    "\n",
    "@sio.on('telemetry')\n",
    "def telemetry(sid, dashboard):\n",
    "    if dashboard:\n",
    "        car.on_dashboard(dashboard)\n",
    "    else:\n",
    "        sio.emit('manual', data={}, skip_sid=True)\n",
    "\n",
    "@sio.on('connect')\n",
    "def connect(sid, environ):\n",
    "    car.control(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app = socketio.Middleware(sio, Flask(__name__))\n",
    "eventlet.wsgi.server(eventlet.listen(('', 4567)), app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_reward(1, 0.2617993877991494)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../Linux/\n",
    "sh StartGame_local.sh &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_angle = math.pi/9\n",
    "angle, step_angle = 0, largest_angle/20.0\n",
    "angles = [angle]\n",
    "\n",
    "while angle < largest_angle:\n",
    "    angle += step_angle\n",
    "    angles.append(angle)\n",
    "    angles.append(angle*-1)\n",
    "print(sorted(angles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "for i in range(10):\n",
    "    pyautogui.press('esc')\n",
    "    #pyautogui.click()\n",
    "    print(i)\n",
    "    sleep(1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
