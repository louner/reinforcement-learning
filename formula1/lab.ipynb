{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pkill -f \"sh StartGame_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sample_bot import *\n",
    "import shutil\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import socketio\n",
    "import eventlet\n",
    "import eventlet.wsgi\n",
    "from flask import Flask\n",
    "\n",
    "import math\n",
    "from queue import Queue\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/louner/school/ml/reinforcement-learning/')\n",
    "from lab import *\n",
    "\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_size = batch_size*10\n",
    "t_max = 5\n",
    "reload_window = 100\n",
    "reward_decay = 0.99\n",
    "last_state = None\n",
    "last_action = None\n",
    "last_reward = None\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 11 possible steering angles, 0 and 5 uniformly selected angle from 0~pi and 0~-1*pi\n",
    "ANGLES = [-3.141592653589793, -2.5132741228718345, -1.8849555921538759, -1.2566370614359172, -0.6283185307179586, 0, 0.6283185307179586, 1.2566370614359172, 1.8849555921538759, 2.5132741228718345, 3.141592653589793]\n",
    "# 3 possible throttle options\n",
    "THROTTLES = [0.0, 0.5, 1.0]\n",
    "ACTIONS = [(angle, throttle) for angle in ANGLES for throttle in THROTTLES] #33 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def send_control(steering_angle, throttle):\n",
    "    sio.emit(\n",
    "        \"steer\",\n",
    "        data={\n",
    "            'steering_angle': str(steering_angle),\n",
    "            'throttle': str(throttle)\n",
    "        },  \n",
    "        skip_sid=True)\n",
    "    \n",
    "def calculate_reward(steering_angle, speed):\n",
    "    return speed * math.cos(steering_angle)\n",
    "\n",
    "class Action:\n",
    "    def __init__(self, action_idx):\n",
    "        self.action_idx = action_idx\n",
    "        self.steering_angle = ACTIONS[action_idx][0]\n",
    "        self.throttle = ACTIONS[action_idx][1]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str({'throttle': self.throttle, 'angle': self.steering_angle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transaction:\n",
    "    columns = ['state', 'action', 'reward', 'next_state', 'advantage']\n",
    "    \n",
    "    def __init__(self, item):\n",
    "        self.item = dict(zip(Transaction.columns, item))\n",
    "    \n",
    "    def  __lt__(self, other):\n",
    "        #sort by time\n",
    "        return self.item['id'] < other.item['id']\n",
    "        #sort by gained label\n",
    "        #return self.item['label'] < other.item['label']\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.item)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        return self.item[key]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.item[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# policy & value network only different at last layer\n",
    "# common part is cnn1-cnn1-fc\n",
    "def build_network():\n",
    "    with tf.variable_scope('Action-Critic-Network-Common', reuse=tf.AUTO_REUSE):\n",
    "        inputs = tf.placeholder(shape=(None, 108, 320, 3), dtype=tf.float32)\n",
    "        layer_1 = slim.conv2d(inputs=inputs, num_outputs=16, kernel_size=(8, 8), stride=(8, 8), padding='VALID', activation_fn=tf.nn.elu)\n",
    "        layer_2 = slim.conv2d(inputs=layer_1, num_outputs=32, kernel_size=(4, 4), stride=(4, 4), padding='VALID', activation_fn=tf.nn.elu)\n",
    "        fc_layer = tf.layers.dense(inputs=slim.flatten(layer_2), units=256, activation=tf.nn.relu)\n",
    "    return inputs, fc_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# P(state) = action probability\n",
    "class PolicyNetwork:\n",
    "    def __init__(self):\n",
    "        self.action_size = 33\n",
    "        \n",
    "        self.advantage = tf.placeholder(shape=(None, self.action_size), dtype=tf.float32)\n",
    "        \n",
    "        inputs, output = build_network()\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        with tf.variable_scope('policy-network'):\n",
    "            self.actions_values = tf.layers.dense(output, units=self.action_size, activation=tf.nn.softmax) # batch_size, action_size\n",
    "            self.best_action = tf.argmax(self.actions_values, axis=1)\n",
    "\n",
    "            self.loss = tf.reduce_sum(tf.log(self.actions_values) * self.advantage)*-1\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        best_action = self.sess.run(self.best_action, feed_dict={self.inputs: [state]})[0]\n",
    "        return best_action\n",
    "    \n",
    "    def update(self, batch):\n",
    "        states = [transaction.state for transaction in batch]\n",
    "        advantages = []\n",
    "        for transaction in batch:\n",
    "            # unselected actions get 0 advantage, selected action get reward - current_state_value advantage\n",
    "            advantage = [0]*self.action_size\n",
    "            advantage[transaction.action] = transaction.advantage\n",
    "\n",
    "            advantages.append(advantage)\n",
    "            \n",
    "        advantages = np.array(advantages)\n",
    "        \n",
    "        return self.sess.run(self.train_op, feed_dict={self.inputs: states, self.advantage: advantages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# V(state) = state_value\n",
    "class ValueNetwork:\n",
    "    def __init__(self):\n",
    "        self.output_size = 1\n",
    "        self.model_folder = './model/value_network'\n",
    "        \n",
    "        self.rewards = tf.placeholder(shape=(None, 1), dtype=tf.float32)\n",
    "        inputs, output = build_network()\n",
    "        \n",
    "        with tf.variable_scope('value-network'):\n",
    "            self.inputs = inputs\n",
    "            self.state_value = tf.layers.dense(output, units=self.output_size)\n",
    "\n",
    "            self.loss = tf.reduce_sum(tf.square(self.state_value - self.rewards))\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.target_sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.target_sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "    \n",
    "    def get_state_values(self, states):\n",
    "        state_values = self.target_sess.run(self.state_value, feed_dict={self.inputs: states})\n",
    "        return state_values\n",
    "    \n",
    "    def get_state_value(self, state):\n",
    "        return self.get_state_values([state])[0]\n",
    "    \n",
    "    def update(self, batch):\n",
    "        inputs = [transaction.state for transaction in batch]\n",
    "        rewards = [transaction.reward for transaction in batch]\n",
    "        \n",
    "        return self.sess.run(self.train_op, feed_dict={self.inputs: inputs, self.rewards: rewards})\n",
    "    \n",
    "    def reload(self, model_name):\n",
    "        self.saver.save(self.sess, '%s/%s'%(self.model_folder, model_name))\n",
    "        self.saver.restore(self.target_sess, tf.train.latest_checkpoint(self.model_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# when t_max transactions are collected, calculate first transaction's reward/advantage\n",
    "def add_advantage(transaction, transactions, value_network):\n",
    "    final_state = transactions[-1].state\n",
    "    final_state_value = value_network.get_state_value(final_state)\n",
    "    \n",
    "    reward = final_state_value\n",
    "    for t in reversed(transactions):\n",
    "        reward += t.reward\n",
    "        reward *= reward_decay\n",
    "        \n",
    "    transaction.reward = reward\n",
    "    \n",
    "    current_state = transaction.state\n",
    "    current_state_value = value_network.get_state_value(current_state)\n",
    "    \n",
    "    advantage = reward - current_state_value\n",
    "    transaction.advantage = advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ACNDrive(AutoDrive):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        AutoDrive.__init__(self, *args, **kwargs)\n",
    "        \n",
    "        self.policy_network = PolicyNetwork()\n",
    "        self.value_network = ValueNetwork()\n",
    "        \n",
    "        self.memory = Memory(max_size=max_size)\n",
    "        self.transactions = []\n",
    "        self.round_id = 1\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "\n",
    "    def on_dashboard(self, src_img, last_steering_angle, speed, throttle, info):        \n",
    "        # get angle to calculate reward\n",
    "        track_img     = ImageProcessor.preprocess(src_img)\n",
    "        current_angle = ImageProcessor.find_steering_angle_by_color(track_img, last_steering_angle, debug = self.debug)\n",
    "        steering_angle = self._steering_pid.update(-current_angle)\n",
    "        throttle       = self._throttle_pid.update(speed)\n",
    "        \n",
    "        # update last transaction\n",
    "        # because can't get next state right after executing action QQ\n",
    "        state = track_img\n",
    "        if self.last_state is not None:\n",
    "            transaction = Transaction([self.last_state, self.last_action, self.last_reward, state, None])\n",
    "            self.transactions.append(transaction)\n",
    "        \n",
    "        # select action according to current state\n",
    "        action = self.policy_network.get_action(state)\n",
    "        action = Action(action)\n",
    "        steering_angle, throttle = action.steering_angle, action.throttle\n",
    "        \n",
    "        self.last_action = action.action_idx\n",
    "        self.last_reward = calculate_reward(steering_angle, speed)\n",
    "        self.last_state = state\n",
    "        \n",
    "        # execute action\n",
    "        self._car.control(steering_angle, throttle)\n",
    "        self.round_id += 1\n",
    "        \n",
    "        # update policy & value network\n",
    "        batch = self.memory.batch(batch_size)\n",
    "        if batch:\n",
    "            self.policy_network.update(batch)\n",
    "            self.value_network.update(batch)\n",
    "        \n",
    "        # when storing t_max transactions, update first transaction's label, pop & push it into memory\n",
    "        if len(self.transactions) == max_size:\n",
    "            transaction = self.transactions[0]\n",
    "            add_advantage(transaction, self.transactions, self.value_network)\n",
    "            self.memory.insert(transaction)\n",
    "            transactions = transactions[1:]\n",
    "        \n",
    "        # reload policy & value network periodically\n",
    "        if self.round_id % reload_window == 0:\n",
    "            #policy_network.reload() #policy network need reload ?\n",
    "            self.value_network.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(17058) wsgi starting up on http://0.0.0.0:4567\n",
      "wsgi exiting\n",
      "(17058) wsgi exited, is_accepting=True\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sio = socketio.Server()\n",
    "record_folder = './records/'\n",
    "car = Car(control_function = send_control)\n",
    "#drive = AutoDrive(car, record_folder=record_folder)\n",
    "drive = ACNDrive(car, record_folder=record_folder)\n",
    "\n",
    "@sio.on('telemetry')\n",
    "def telemetry(sid, dashboard):\n",
    "    if dashboard:\n",
    "        car.on_dashboard(dashboard)\n",
    "    else:\n",
    "        sio.emit('manual', data={}, skip_sid=True)\n",
    "\n",
    "@sio.on('connect')\n",
    "def connect(sid, environ):\n",
    "    car.control(0, 0)\n",
    "\n",
    "app = socketio.Middleware(sio, Flask(__name__))\n",
    "eventlet.wsgi.server(eventlet.listen(('', 4567)), app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../Linux/\n",
    "sh StartGame_local.sh &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.141592653589793, -2.5132741228718345, -1.8849555921538759, -1.2566370614359172, -0.6283185307179586, 0, 0.6283185307179586, 1.2566370614359172, 1.8849555921538759, 2.5132741228718345, 3.141592653589793]\n"
     ]
    }
   ],
   "source": [
    "angle, step_angle = 0, math.pi/5.0\n",
    "angles = [angle]\n",
    "\n",
    "while angle < math.pi:\n",
    "    angle += step_angle\n",
    "    angles.append(angle)\n",
    "    angles.append(angle*-1)\n",
    "print(sorted(angles))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
