{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from random import randint, choice\n",
    "from gym.envs.classic_control import CartPoleEnv\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "from random import random, sample\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "T = batch_size*10\n",
    "GAMMA = 0.99\n",
    "learning_rate = 0.001\n",
    "update_target_network_time = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Transaction:\n",
    "    columns = ['state', 'action', 'reward', 'next_state', 'done', 'label']\n",
    "    actions = [[0, 1], [1, 0]]\n",
    "    def __init__(self, item):\n",
    "        self.item = dict(zip(Transaction.columns, item))\n",
    "        \n",
    "    def  __lt__(self, other):\n",
    "        #sort by time\n",
    "        return self.item['id'] < other.item['id']\n",
    "        #sort by gained label\n",
    "        #return self.item['label'] < other.item['label']\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.item)\n",
    "    \n",
    "    def get_next_state_and_(self):\n",
    "        return self.item['state'] + Transaction.actions[self.item['action']]\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.item[key]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.item[key] = value\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        MAX_TRANSACTION = Transaction(Transaction.columns)\n",
    "        MAX_TRANSACTION.item['id'] = -1\n",
    "        self.d = []\n",
    "        self.id = 0\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def insert(self, item):\n",
    "        self.id += 1\n",
    "        item['id'] = self.id\n",
    "        #print(self.d)\n",
    "\n",
    "        if len(self.d) == self.max_size:\n",
    "            heapq.heapreplace(self.d, item)\n",
    "        else:\n",
    "            heapq.heappush(self.d, item)\n",
    "\n",
    "    def batch(self, n):\n",
    "        if len(self.d) > n:\n",
    "            batch = np.array(sample(self.d, n))\n",
    "            return batch\n",
    "        else:\n",
    "            return np.array(self.d)\n",
    "\n",
    "def get_labels(batch):\n",
    "    return np.array([transaction['label'] for transaction in batch])\n",
    "\n",
    "def test():\n",
    "    D = Memory(2)\n",
    "    item1 = Transaction([np.array([0,1,2,3]), 1,0,np.array([3,2,1,0]),0,10])\n",
    "    item2 = Transaction([np.array([0,1,2,3]), 1,0,np.array([3,2,1,0]),0,10])\n",
    "    item3 = Transaction([np.array([0,1,2,3]), 1,0,np.array([3,2,1,0]),0,10])\n",
    "\n",
    "    D.insert(item1)\n",
    "    D.insert(item2)\n",
    "    D.insert(item3)\n",
    "\n",
    "    print(D.d, D.batch(2), D.d)\n",
    "    item = Transaction([np.array([-1,1,2,3]), 1,0,np.array([3,2,1,0]),0,10])\n",
    "    D.insert(item)\n",
    "    print(D.d, D.batch(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self):\n",
    "        self.state_size = 4\n",
    "        self.action_size = 2\n",
    "        self.units = 32\n",
    "        self.n_layers = 4\n",
    "        self.model_folder = './model/'\n",
    "        \n",
    "        self.input = tf.placeholder(shape=(None, self.state_size), dtype=tf.float32)\n",
    "        self.label = tf.placeholder(shape=(None, self.action_size), dtype=tf.float32)\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32)\n",
    "        \n",
    "        self.build_network()\n",
    "        self.sess = sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 1}))\n",
    "        self.target_Q_sess = sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 1}))\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.target_Q_sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def build_network(self):\n",
    "        layers = [0]*self.n_layers\n",
    "        layers[0] = tf.layers.dense(inputs=self.input, units=self.units, activation=tf.nn.relu, name='layer_0')\n",
    "        for i in range(self.n_layers-1):\n",
    "            layers[i+1] = tf.layers.dense(inputs=layers[i], units=self.units, activation=tf.nn.relu, name='layer_%d'%(i+1))\n",
    "        \n",
    "        self.qvalue = tf.layers.dense(inputs=layers[-1], units=self.action_size)\n",
    "        self.best_qvalue = tf.reduce_max(self.qvalue, axis=1)\n",
    "\n",
    "        self.loss = tf.losses.mean_squared_error(labels=self.label, predictions=self.qvalue)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self.train_step = self.do_clipping(self.loss, optimizer)\n",
    "        #self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        self.best_action = tf.argmax(self.qvalue, axis=1)\n",
    "        \n",
    "    def do_clipping(self, loss, optimizer):\n",
    "        grad_vars = optimizer.compute_gradients(loss)\n",
    "        self.grad_vars = [\n",
    "          (tf.clip_by_norm(grad, clip_norm=10), var)\n",
    "          for grad, var in grad_vars if grad is not None\n",
    "        ]\n",
    "        self.grad_dict = dict([\n",
    "          [var.name, grad]\n",
    "          for grad, var in self.grad_vars\n",
    "        ])\n",
    "\n",
    "        train_step = optimizer.apply_gradients(self.grad_vars, global_step=self.global_step)\n",
    "        return train_step\n",
    "\n",
    "    def get_Q_value(self, states, sess='target'):\n",
    "        sess = self.decide_sess(sess)\n",
    "        return sess.run(self.qvalue, feed_dict={self.input: states})\n",
    "    \n",
    "    def get_best_Q_value(self, states, sess='target'):\n",
    "        sess = self.decide_sess(sess)\n",
    "        return sess.run(self.best_qvalue, feed_dict={self.input: states})\n",
    "    \n",
    "    def get_best_action(self, state, sess='target'):\n",
    "        sess = self.decide_sess(sess)\n",
    "        return sess.run(self.best_action, feed_dict={self.input: [state]})[0]\n",
    "            \n",
    "    def update(self, batch):\n",
    "        states = get_states(batch)\n",
    "        labels = get_labels(batch)\n",
    "        \n",
    "        return self.sess.run([self.train_step, self.loss], feed_dict={self.input: states, self.label: labels})\n",
    "    \n",
    "    def reload_target_Q_network(self):\n",
    "        self.saver.restore(self.target_Q_sess, tf.train.latest_checkpoint(self.model_folder))\n",
    "        \n",
    "    def save(self):\n",
    "        self.saver.save(self.sess, self.model_folder, global_step=self.global_step)\n",
    "        \n",
    "    def decide_sess(self, sess):\n",
    "        if sess == 'target':\n",
    "            sess = self.target_Q_sess\n",
    "        elif sess == 'online':\n",
    "            sess = self.sess\n",
    "        else:\n",
    "            raise Exception('unknown sess %s'%(sess))\n",
    "        return sess\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_to_Q_input(states, actions):\n",
    "    action_onehots = [Transaction.actions[action] for action in actions]\n",
    "    Q_inputs = np.array([state + action_onehot for state, action_onehot in zip(states, action_onehots)])\n",
    "    return Q_inputs\n",
    "\n",
    "def get_next_states(batch):\n",
    "    next_states = [transaction['next_state'] for transaction in batch]\n",
    "    return next_states\n",
    "\n",
    "def get_states(batch):\n",
    "    states = [transaction['state'] for transaction in batch]\n",
    "    return states\n",
    "\n",
    "def get_labels(batch):\n",
    "    labels = [transaction['label'] for transaction in batch]\n",
    "    return labels\n",
    "\n",
    "def get_actions(batch):\n",
    "    actions = [transaction['action'] for transaction in batch]\n",
    "    return actions\n",
    "\n",
    "def add_label(batch, Q, gamma):\n",
    "    states = get_states(batch)\n",
    "    states_qvalues = Q.get_Q_value(states, 'target') #batch_size,action_size\n",
    "    \n",
    "    next_states = get_next_states(batch)\n",
    "    next_states_best_qvalues = Q.get_best_Q_value(next_states) #batch_size,1\n",
    "    \n",
    "    for transaction, state_qvalue, next_state_best_qvalue in zip(batch, states_qvalues, next_states_best_qvalues):\n",
    "        if transaction['done']:\n",
    "            reward = transaction['reward']\n",
    "        else:\n",
    "            reward = transaction['reward'] + GAMMA*next_state_best_qvalue\n",
    "            \n",
    "        label = state_qvalue\n",
    "        label[transaction['action']] = reward\n",
    "        \n",
    "        transaction['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot(metrics):\n",
    "    x = [i for i in range(len(metrics['action'][0]))]\n",
    "    #plt.plot(x, metrics['loss'], 'b', x, metrics['action'][0], 'r', x, metrics['action'][1], 'y')\n",
    "    #plt.plot(x, metrics['loss'], 'b')\n",
    "    plt.plot(x, metrics['action'][0], 'r', x, metrics['action'][1], 'y')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "D = Memory(T)\n",
    "Q = QNetwork()\n",
    "\n",
    "env = CartPoleEnv()\n",
    "#env.theta_threshold_radians = 2 * math.pi * 1/4\n",
    "state = env.reset()\n",
    "state = state.tolist()\n",
    "\n",
    "metrics  = {}\n",
    "metrics['loss'] = [0]\n",
    "metrics['action'] = [[0], [0]]\n",
    "metrics['best_action'] = [[0], [0]]\n",
    "metrics['round_length'] = []\n",
    "\n",
    "t = 0\n",
    "one_round = []\n",
    "mu = 1.0\n",
    "\n",
    "for rid in range(2000000):\n",
    "    #env.render()\n",
    "    if random() < mu:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = Q.get_best_action(state, 'online')\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action) # take a random action\n",
    "    next_state = next_state.tolist()\n",
    "    \n",
    "    transaction = Transaction((state, action, reward, next_state, done, -1))\n",
    "    one_round.append(transaction)\n",
    "    state = next_state\n",
    "    \n",
    "    '''\n",
    "    best_action = Q.get_best_action(transaction['state'], 'online')\n",
    "    action = env.action_space.sample()\n",
    "    metrics['action'][action].append(metrics['action'][action][-1]+1)\n",
    "    metrics['action'][action ^ 1].append(metrics['action'][action ^ 1][-1])\n",
    "    \n",
    "    metrics['best_action'][best_action].append(metrics['best_action'][best_action][-1]+1)\n",
    "    metrics['best_action'][best_action ^ 1].append(metrics['best_action'][best_action ^ 1][-1])\n",
    "    '''\n",
    "\n",
    "    batch = D.batch(batch_size)\n",
    "    if batch.any():\n",
    "        add_label(batch, Q, GAMMA)\n",
    "        _, loss = Q.update(batch)\n",
    "        \n",
    "        metrics['loss'].append(loss)\n",
    "    \n",
    "    #print(transaction, batch)    \n",
    "    if done:\n",
    "        reward = 0\n",
    "        for transaction in reversed(one_round):\n",
    "            reward += transaction['reward']\n",
    "            transaction['reward'] = reward\n",
    "            \n",
    "        for transaction in one_round:\n",
    "            D.insert(transaction)\n",
    "            \n",
    "        metrics['round_length'].append(len(one_round))\n",
    "        one_round = []\n",
    "            \n",
    "        state = env.reset()\n",
    "        state = state.tolist()\n",
    "        \n",
    "        mu *= 0.993\n",
    "\n",
    "\n",
    "    if rid > 0 and rid % update_target_network_time == 0:\n",
    "        Q.save()\n",
    "        Q.reload_target_Q_network()\n",
    "        \n",
    "    if rid > 0 and rid % 1000 == 0:\n",
    "        #plt.plot(metrics['action'][0], metrics['action'][1], 'g', metrics['best_action'][0], metrics['best_action'][1], 'y')        \n",
    "        print(mu, rid, metrics['round_length'][-1])\n",
    "        \n",
    "        length = metrics['round_length']\n",
    "        #range_size = max(1, len(length)/100)\n",
    "        range_size = 100\n",
    "        #length = [np.mean(length[i:i+range_size]) for i in range(0, len(length), range_size)]\n",
    "        plt.gcf().clear()\n",
    "        plt.plot(length, 'g')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = Q.get_best_action(state)\n",
    "    #action = env.action_space.sample()\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action) # take a random action\n",
    "    state = next_state.tolist()\n",
    "    print(next_state, reward, action, _)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "41/112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
